{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This pipeline is meant to be run on a kubernetes cluster only, Do not run on your local machine directly to avoid causing complications in your development environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/jovyan/.local/lib/python3.6/site-packages (20.3.1)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[31mERROR: Invalid requirement: 'tensorflow==2.3.0,'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --user --upgrade pip\n",
    "\n",
    "!pip3 install spacy tensorflow==2.3.0, keras==2.2.4 pandas==0.24.2 matplotlib==3.2.2 scipy==1.4.1 statsmodels==0.12.0 scikit-learn==0.23.1 --user\n",
    "\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting https://storage.googleapis.com/ml-pipeline/release/0.1.10/kfp.tar.gz\n",
      "  Using cached https://storage.googleapis.com/ml-pipeline/release/0.1.10/kfp.tar.gz (129 kB)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (1.25.7)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (1.13.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (2.8.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (5.3)\n",
      "Requirement already satisfied: google-cloud-storage==1.13.0 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp==0.1) (1.13.0)\n",
      "Requirement already satisfied: kubernetes==8.0.0 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp==0.1) (8.0.0)\n",
      "Requirement already satisfied: PyJWT==1.6.4 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp==0.1) (1.6.4)\n",
      "Requirement already satisfied: cryptography==2.4.2 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp==0.1) (2.4.2)\n",
      "Requirement already satisfied: requests_toolbelt==0.8.0 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp==0.1) (0.8.0)\n",
      "Requirement already satisfied: idna>=2.1 in /usr/lib/python3/dist-packages (from cryptography==2.4.2->kfp==0.1) (2.6)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /usr/lib/python3/dist-packages (from cryptography==2.4.2->kfp==0.1) (0.24.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (1.13.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.7 in /home/jovyan/.local/lib/python3.6/site-packages (from cryptography==2.4.2->kfp==0.1) (1.14.4)\n",
      "Requirement already satisfied: pycparser in /home/jovyan/.local/lib/python3.6/site-packages (from cffi!=1.11.3,>=1.7->cryptography==2.4.2->kfp==0.1) (2.20)\n",
      "Collecting google-auth==1.6.1\n",
      "  Using cached google_auth-1.6.1-py2.py3-none-any.whl (68 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth==1.6.1->kfp==0.1) (0.2.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth==1.6.1->kfp==0.1) (4.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (1.13.0)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth==1.6.1->kfp==0.1) (4.0.0)\n",
      "Requirement already satisfied: google-resumable-media>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage==1.13.0->kfp==0.1) (0.5.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage==1.13.0->kfp==0.1) (1.16.0)\n",
      "Requirement already satisfied: google-cloud-core<0.29dev,>=0.28.0 in /home/jovyan/.local/lib/python3.6/site-packages (from google-cloud-storage==1.13.0->kfp==0.1) (0.28.1)\n",
      "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (44.0.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (3.11.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (1.51.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (2019.3)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (1.13.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (2.22.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage==1.13.0->kfp==0.1) (1.16.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (1.13.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (3.11.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (2.22.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes==8.0.0->kfp==0.1) (1.3.0)\n",
      "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (44.0.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (2019.11.28)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes==8.0.0->kfp==0.1) (0.57.0)\n",
      "Requirement already satisfied: adal>=1.0.2 in /home/jovyan/.local/lib/python3.6/site-packages (from kubernetes==8.0.0->kfp==0.1) (1.2.5)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (5.3)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (1.13.0)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (1.25.7)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (2.8.1)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (2.8.1)\n",
      "Requirement already satisfied: cryptography==2.4.2 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp==0.1) (2.4.2)\n",
      "Requirement already satisfied: PyJWT==1.6.4 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp==0.1) (1.6.4)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (2.22.0)\n",
      "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (44.0.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (1.13.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth==1.6.1->kfp==0.1) (0.4.8)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (1.13.0)\n",
      "Requirement already satisfied: idna>=2.1 in /usr/lib/python3/dist-packages (from cryptography==2.4.2->kfp==0.1) (2.6)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (1.25.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes==8.0.0->kfp==0.1) (3.1.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (2.22.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (2.22.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth==1.6.1->kfp==0.1) (0.4.8)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kfp==0.1) (1.13.0)\n",
      "Building wheels for collected packages: kfp\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-0.1-py3-none-any.whl size=211223 sha256=690972feaeec81ce850df81bc9ac1457bc695b130c3576d5a0138c5a23928a10\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3ap_goj8/wheels/98/cf/56/0c692ab36d3472d515b7b973fab48add9214110e4bdcd0ca7f\n",
      "Successfully built kfp\n",
      "Installing collected packages: google-auth, kfp\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 1.24.0\n",
      "    Uninstalling google-auth-1.24.0:\n",
      "      Successfully uninstalled google-auth-1.24.0\n",
      "  Attempting uninstall: kfp\n",
      "    Found existing installation: kfp 1.1.2\n",
      "    Uninstalling kfp-1.1.2:\n",
      "      Successfully uninstalled kfp-1.1.2\n",
      "\u001b[33m  WARNING: The script dsl-compile is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.4.0 requires google-auth<2,>=1.6.3, but you have google-auth 1.6.1 which is incompatible.\n",
      "fairing 0.5 requires google-auth>=1.6.2, but you have google-auth 1.6.1 which is incompatible.\n",
      "fairing 0.5 requires google-cloud-storage>=1.13.2, but you have google-cloud-storage 1.13.0 which is incompatible.\n",
      "fairing 0.5 requires kubernetes>=9.0.0, but you have kubernetes 8.0.0 which is incompatible.\u001b[0m\n",
      "Successfully installed google-auth-1.6.1 kfp-0.1\n",
      "Requirement already satisfied: kfp in /home/jovyan/.local/lib/python3.6/site-packages (0.1)\n",
      "Collecting kfp\n",
      "  Using cached kfp-1.1.2-py3-none-any.whl\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (1.13.0)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (0.8.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from kfp) (1.2.2)\n",
      "Requirement already satisfied: strip-hints in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (8.0.0)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.1b1 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (1.1.2rc1)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: docstring-parser>=0.7.3 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (0.7.3)\n",
      "Requirement already satisfied: Deprecated in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (1.2.10)\n",
      "Requirement already satisfied: click in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from kfp) (5.3)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (1.6.1)\n",
      "Requirement already satisfied: tabulate in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (0.8.7)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.0 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (0.1.3.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from Deprecated->kfp) (1.11.2)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: google-cloud-core<0.29dev,>=0.28.0 in /home/jovyan/.local/lib/python3.6/site-packages (from google-cloud-storage>=1.13.0->kfp) (0.28.1)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: google-resumable-media>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (0.5.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage>=1.13.0->kfp) (2019.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage>=1.13.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (1.6.1)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage>=1.13.0->kfp) (3.11.2)\n",
      "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage>=1.13.0->kfp) (44.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage>=1.13.0->kfp) (1.51.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage>=1.13.0->kfp) (3.11.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (19.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (0.15.7)\n",
      "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage>=1.13.0->kfp) (44.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (1.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonschema>=3.0.1->kfp) (0.6.0)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2.8.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (1.25.7)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2019.11.28)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2019.11.28)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from kfp) (5.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied: adal>=1.0.2 in /home/jovyan/.local/lib/python3.6/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.2.5)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (1.6.1)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (1.25.7)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2.8.1)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage>=1.13.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage>=1.13.0->kfp) (44.0.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage>=1.13.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: cryptography>=1.1.0 in /home/jovyan/.local/lib/python3.6/site-packages (from adal>=1.0.2->kubernetes<12.0.0,>=8.0.0->kfp) (2.4.2)\n",
      "Requirement already satisfied: PyJWT>=1.0.0 in /home/jovyan/.local/lib/python3.6/site-packages (from adal>=1.0.2->kubernetes<12.0.0,>=8.0.0->kfp) (1.6.4)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2.8.1)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.7 in /home/jovyan/.local/lib/python3.6/site-packages (from cryptography>=1.1.0->adal>=1.0.2->kubernetes<12.0.0,>=8.0.0->kfp) (1.14.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: idna>=2.1 in /usr/lib/python3/dist-packages (from cryptography>=1.1.0->adal>=1.0.2->kubernetes<12.0.0,>=8.0.0->kfp) (2.6)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /usr/lib/python3/dist-packages (from cryptography>=1.1.0->adal>=1.0.2->kubernetes<12.0.0,>=8.0.0->kfp) (0.24.0)\n",
      "Requirement already satisfied: pycparser in /home/jovyan/.local/lib/python3.6/site-packages (from cffi!=1.11.3,>=1.7->cryptography>=1.1.0->adal>=1.0.2->kubernetes<12.0.0,>=8.0.0->kfp) (2.20)\n",
      "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage>=1.13.0->kfp) (44.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage>=1.13.0->kfp) (3.0.4)\n",
      "Requirement already satisfied: idna>=2.1 in /usr/lib/python3/dist-packages (from cryptography>=1.1.0->adal>=1.0.2->kubernetes<12.0.0,>=8.0.0->kfp) (2.6)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (1.25.7)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2019.11.28)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.1.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage>=1.13.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage>=1.13.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints->kfp) (0.30.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata->jsonschema>=3.0.1->kfp) (8.0.2)\n",
      "Installing collected packages: kfp\n",
      "  Attempting uninstall: kfp\n",
      "    Found existing installation: kfp 0.1\n",
      "    Uninstalling kfp-0.1:\n",
      "      Successfully uninstalled kfp-0.1\n",
      "\u001b[33m  WARNING: The scripts dsl-compile, dsl-compile-v2 and kfp are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed kfp-1.1.2\n"
     ]
    }
   ],
   "source": [
    "# Install or Upgrade if present the KFP library\n",
    "\n",
    "!python3 -m pip install https://storage.googleapis.com/ml-pipeline/release/0.1.10/kfp.tar.gz --upgrade\n",
    "\n",
    "!python3 -m pip install --upgrade kfp --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the install was successful\n",
    "\n",
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kubeflow SDK\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "\n",
    "\n",
    "# where the outputs are stored\n",
    "data_path = \"pipe_data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy(data_path):\n",
    "    \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'spacy==2.2.0'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'keras==2.2.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'ipython'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz'])\n",
    "    \n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    from IPython import get_ipython\n",
    "    from math import sqrt\n",
    "    from numpy import concatenate\n",
    "    from pandas import read_csv, DataFrame, concat\n",
    "    from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.utils import to_categorical\n",
    "    from keras import regularizers\n",
    "    from keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import spacy\n",
    "    \n",
    "    try:\n",
    "        # check if the model configuration files are present in the pipe_data directory, if it isnt we would build our model and add them \n",
    "        arc = os.path.join(os.getcwd(),'pipe_data/model.json')\n",
    "        json_file = open(arc, 'r')\n",
    "        \n",
    "    except:\n",
    "    \n",
    "        data = pd.read_csv('https://raw.githubusercontent.com/Chizzy-codes/g02-sms-spam/master/data/spam.csv', usecols=['v1', 'v2'],  encoding = 'latin-1')\n",
    "        data2 = pd.read_csv('https://raw.githubusercontent.com/Chizzy-codes/g02-sms-spam/master/data/spam_additional.csv', usecols=['Text', 'v3'],  encoding = 'latin-1')\n",
    "\n",
    "        embedding_dim = 100\n",
    "        max_length = 150\n",
    "        trunc_type='post'\n",
    "        padding_type='post'\n",
    "        oov_tok = \"<OOV>\"\n",
    "\n",
    "\n",
    "        data.dropna(inplace=True,axis=1)\n",
    "        data.rename(columns={\"v1\":\"label\",\"v2\":\"message\"},inplace=True)\n",
    "        data2 = data2[['Text','v3']].copy()\n",
    "        data2.rename(columns={\"v3\":\"label\",\"Text\":\"message\"},inplace=True)\n",
    "\n",
    "        # merge the datframes\n",
    "        data = data.append(data2)\n",
    "\n",
    "        # To drop duplicated rows\n",
    "        data.drop_duplicates(inplace=True)\n",
    "\n",
    "        # shuffle rows\n",
    "        data = data.sample(frac = 1, random_state=44).reset_index(drop=True)\n",
    "\n",
    "        nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner'])\n",
    "        spacy_stop_words = spacy.lang.en.STOP_WORDS # getting spacy's stop-words\n",
    "\n",
    "        #Stopwords list from https://github.com/Yoast/YoastSEO.js/blob/develop/src/config/stopwords.js\n",
    "        # Convert it to a Python list and paste it here\n",
    "        stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "\n",
    "        stop_words = list(set(list(spacy_stop_words) + stopwords))\n",
    "        stopwords = list({word.lemma_.lower() for word in nlp(' '.join(stop_words))})\n",
    "\n",
    "        # defining tokenzer function to tokenize the lower case lemma of documents in a corpus and \n",
    "        # filter out stop-words  \n",
    "        def tokenizer_spacy(text):\n",
    "            return [word.lemma_.lower() for word in nlp(text) if word.is_alpha and word.lemma_.lower() not in stopwords]\n",
    "\n",
    "        #Replace the target colunms with binary number\n",
    "        data.replace({'ham':1,'spam':0},inplace=True)\n",
    "\n",
    "        sentences = data['message']\n",
    "        labels=data['label']\n",
    "\n",
    "        #Replace the target colunms with binary number\n",
    "        data.replace({'ham':1,'spam':0},inplace=True)\n",
    "\n",
    "        senten = [word for word in sentences if word not in stopwords] # stopword filtering\n",
    "\n",
    "        # tokenize (lemmatize and filter stop words) corpus \n",
    "        senten = [' '.join(tokenizer_spacy(doc)) for doc in senten]\n",
    "\n",
    "        # word tokenizing\n",
    "        tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "        tokenizer.fit_on_texts(senten)\n",
    "\n",
    "        word_index = tokenizer.word_index\n",
    "\n",
    "        vocab_size=len(word_index)\n",
    "\n",
    "        # padding and converting to numeric sequence\n",
    "        sequences = tokenizer.texts_to_sequences(sentences)\n",
    "        padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        training_sequences, test_sequences, training_labels, test_labels = train_test_split(padded, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "        from sklearn.utils import class_weight\n",
    "        class_weights = dict(zip(np.unique(labels), class_weight.compute_class_weight('balanced',\n",
    "                                                     np.unique(labels),\n",
    "                                                     labels))) \n",
    "\n",
    "        # Modelling\n",
    "        tf.random.set_seed(1) # for reproducibility\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(305)),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        model.summary()\n",
    "\n",
    "        num_epochs = 12\n",
    "\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                       patience=5,\n",
    "                                       verbose=1,\n",
    "                                       min_delta=1e-4)\n",
    "\n",
    "        lrp = tf.keraslrp = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                           factor=0.1,\n",
    "                                           patience=2,\n",
    "                                           cooldown=2,\n",
    "                                           verbose=1)\n",
    "\n",
    "        training_padded = np.array(training_sequences)\n",
    "        training_labels = np.array(training_labels)\n",
    "        testing_padded = np.array(test_sequences)\n",
    "        testing_labels = np.array(test_labels)\n",
    "\n",
    "        history = model.fit(training_padded, training_labels, epochs=num_epochs, \n",
    "                            class_weight=class_weights, \n",
    "                            validation_data=(testing_padded, testing_labels),\n",
    "                            callbacks=[early_stopping, lrp],\n",
    "                            verbose=2)\n",
    "\n",
    "        print(\"Training Complete\")\n",
    "\n",
    "        # serialize model to JSON\n",
    "\n",
    "        model_json = model.to_json()\n",
    "\n",
    "        with open(f'{data_path}/model.json', 'w') as json_file:\n",
    "            json_file.write(model_json)\n",
    "\n",
    "        # serialise weights to HDF5\n",
    "        model.save_weights(f'{data_path}/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jovyan/.local/lib/python3.6/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n",
      "/home/jovyan/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass classes=[0 1], y=0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "5       1\n",
      "6       1\n",
      "7       1\n",
      "8       1\n",
      "9       1\n",
      "10      1\n",
      "11      1\n",
      "12      1\n",
      "13      1\n",
      "14      1\n",
      "15      1\n",
      "16      1\n",
      "17      1\n",
      "18      1\n",
      "19      1\n",
      "20      1\n",
      "21      1\n",
      "22      1\n",
      "23      1\n",
      "24      1\n",
      "25      1\n",
      "26      1\n",
      "27      1\n",
      "28      1\n",
      "29      1\n",
      "       ..\n",
      "5572    0\n",
      "5573    1\n",
      "5574    1\n",
      "5575    1\n",
      "5576    1\n",
      "5577    1\n",
      "5578    1\n",
      "5579    1\n",
      "5580    1\n",
      "5581    0\n",
      "5582    0\n",
      "5583    1\n",
      "5584    1\n",
      "5585    1\n",
      "5586    1\n",
      "5587    0\n",
      "5588    1\n",
      "5589    1\n",
      "5590    1\n",
      "5591    1\n",
      "5592    0\n",
      "5593    1\n",
      "5594    1\n",
      "5595    1\n",
      "5596    1\n",
      "5597    1\n",
      "5598    1\n",
      "5599    1\n",
      "5600    1\n",
      "5601    1\n",
      "Name: label, Length: 5602, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 150, 100)          763600    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 150, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 610)               990640    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 610)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 611       \n",
      "=================================================================\n",
      "Total params: 1,754,851\n",
      "Trainable params: 1,754,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 4201 samples, validate on 1401 samples\n",
      "Epoch 1/12\n",
      "4201/4201 - 60s - loss: 0.4315 - accuracy: 0.7746 - val_loss: 0.1783 - val_accuracy: 0.9593\n",
      "Epoch 2/12\n",
      "4201/4201 - 56s - loss: 0.1197 - accuracy: 0.9707 - val_loss: 0.1111 - val_accuracy: 0.9793\n",
      "Epoch 3/12\n",
      "4201/4201 - 56s - loss: 0.0524 - accuracy: 0.9900 - val_loss: 0.0944 - val_accuracy: 0.9836\n",
      "Epoch 4/12\n",
      "4201/4201 - 56s - loss: 0.0399 - accuracy: 0.9919 - val_loss: 0.0768 - val_accuracy: 0.9764\n",
      "Epoch 5/12\n",
      "4201/4201 - 56s - loss: 0.0192 - accuracy: 0.9950 - val_loss: 0.1100 - val_accuracy: 0.9829\n",
      "Epoch 6/12\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "4201/4201 - 56s - loss: 0.0459 - accuracy: 0.9876 - val_loss: 0.0906 - val_accuracy: 0.9829\n",
      "Epoch 7/12\n",
      "4201/4201 - 57s - loss: 0.0275 - accuracy: 0.9971 - val_loss: 0.0917 - val_accuracy: 0.9836\n",
      "Epoch 8/12\n",
      "4201/4201 - 56s - loss: 0.0225 - accuracy: 0.9967 - val_loss: 0.0902 - val_accuracy: 0.9857\n",
      "Epoch 9/12\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "4201/4201 - 56s - loss: 0.0173 - accuracy: 0.9974 - val_loss: 0.0996 - val_accuracy: 0.9836\n",
      "Epoch 00009: early stopping\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "run = deploy(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Injestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_injestion(data_path):\n",
    "    global loaded_model\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    \n",
    "    import pandas as pd\n",
    "    import tensorflow\n",
    "    \n",
    "    data = pd.read_csv('https://raw.githubusercontent.com/Chizzy-codes/g02-sms-spam/master/data/main_data.csv', usecols=['label', 'message'],  encoding = 'latin-1')\n",
    "  \n",
    "    #Save the injested data as a pickle file to be used by the data tranformation component.\n",
    "    with open(f'{data_path}/inj_data', 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = data_injestion(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transformation(data_path):\n",
    "    \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'spacy==2.2.0'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'keras==2.2.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'ipython'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz'])\n",
    "\n",
    "    \n",
    "    from IPython import get_ipython\n",
    "    from math import sqrt\n",
    "    from numpy import concatenate\n",
    "    from pandas import read_csv, DataFrame, concat\n",
    "    from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.utils import to_categorical\n",
    "    from keras import regularizers\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import spacy\n",
    "    \n",
    "    \n",
    "    embedding_dim = 100\n",
    "    max_length = 150\n",
    "    trunc_type='post'\n",
    "    padding_type='post'\n",
    "    oov_tok = \"<OOV>\"\n",
    "    \n",
    "    # Load and unpack the data\n",
    "    \n",
    "    with open(f'{data_path}/inj_data','rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner'])\n",
    "    spacy_stop_words = spacy.lang.en.STOP_WORDS # getting spacy's stop-words\n",
    "\n",
    "    #Stopwords list from https://github.com/Yoast/YoastSEO.js/blob/develop/src/config/stopwords.js\n",
    "    # Convert it to a Python list and paste it here\n",
    "    stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "\n",
    "    stop_words = list(set(list(spacy_stop_words) + stopwords))\n",
    "    stopwords = list({word.lemma_.lower() for word in nlp(' '.join(stop_words))})\n",
    "\n",
    "    # defining tokenzer function to tokenize the lower case lemma of documents in a corpus and \n",
    "    # filter out stop-words  \n",
    "    def tokenizer_spacy(text):\n",
    "        return [word.lemma_.lower() for word in nlp(text) if word.is_alpha and word.lemma_.lower() not in stopwords]\n",
    "\n",
    "    #Replace the target colunms with binary number\n",
    "    data.replace({'ham':1,'spam':0},inplace=True)\n",
    "\n",
    "    sentences = data['message']\n",
    "    \n",
    "    try:\n",
    "        labels = data['label']\n",
    "    except:\n",
    "        label = None\n",
    "    else:\n",
    "        labels = data['label']\n",
    "\n",
    "    #Replace the target colunms with binary number\n",
    "    #data.replace({'ham':1,'spam':0},inplace=True)\n",
    "\n",
    "    senten = [word for word in sentences if word not in stopwords] # stopword filtering\n",
    "\n",
    "    # tokenize (lemmatize and filter stop words) corpus \n",
    "    senten = [' '.join(tokenizer_spacy(doc)) for doc in senten]\n",
    "\n",
    "    # word tokenizing\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(senten)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    vocab_size=len(word_index)\n",
    "\n",
    "    # padding and converting to numeric sequence\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "\n",
    "    padded = np.array(padded)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    #Save the preprocessed data as a pickle file to be used by the model component.\n",
    "    with open(f'{data_path}/preprocessed_data', 'wb') as f:\n",
    "        pickle.dump((padded, labels), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data_transformation(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_path):\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.3.0'])\n",
    "    \n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import tensorflow\n",
    "    \n",
    "    from sklearn.metrics import f1_score, precision_score\n",
    "    import os\n",
    "    \n",
    "    with open(f'{data_path}/preprocessed_data','rb') as f:\n",
    "        padded, labels = pickle.load(f)\n",
    "    \n",
    "    \n",
    "    arc = os.path.join(os.getcwd(),'pipe_data/model.json')\n",
    "    weights = os.path.join(os.getcwd(),'pipe_data/model.h5')\n",
    "\n",
    "    # load json and create model\n",
    "    #json_file = open('https://raw.githubusercontent.com/Chizzy-codes/g02-sms-spam/master/pipeline/pipe_data/model.json', 'r')\n",
    "    json_file = open(arc, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = tensorflow.keras.models.model_from_json(loaded_model_json)\n",
    "\n",
    "    # load weights into new model\n",
    "    #loaded_model.load_weights('pipe_data/model.h5')\n",
    "    loaded_model.load_weights(weights)\n",
    "\n",
    "    # compile loaded model\n",
    "\n",
    "    loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    \n",
    "     # Load and unpack the test_data\n",
    "    \n",
    "    \n",
    "    \n",
    "    prediction = loaded_model.predict(padded)\n",
    "       \n",
    "    prediction = [int(np.round(i)) for i in prediction]\n",
    "           \n",
    "    with open(f'{data_path}/result', 'wb') as f:\n",
    "        pickle.dump(prediction, f)\n",
    "    \n",
    "    print('Successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully!\n"
     ]
    }
   ],
   "source": [
    "predicted = predict(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor(data_path):\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    \n",
    "    from sklearn.metrics import f1_score, precision_score\n",
    "    \n",
    "    with open(f'{data_path}/preprocessed_data','rb') as f:\n",
    "        padded, labels = pickle.load(f)\n",
    "    \n",
    "    \n",
    "    with open(f'{data_path}/result', 'rb') as f:\n",
    "        prediction = pickle.load(f)\n",
    "        \n",
    "    evaluation = []\n",
    "        \n",
    "    if labels.all() != None:\n",
    "    \n",
    "        f1 = 'F1 score: {:.4f}'.format(f1_score(labels, prediction))\n",
    "\n",
    "        precision = 'Precision_score: {:.4f}'.format(precision_score(labels, prediction))\n",
    "    \n",
    "        evaluation.append(f1)\n",
    "        evaluation.append(precision)\n",
    "    \n",
    "    with open(f'{data_path}/result.txt', 'w') as f:\n",
    "        f.write(\" Evaluation: {}\".format(evaluation))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\" Predictions: {}\".format(prediction))\n",
    "        \n",
    "        \n",
    "    print('Done!!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "get = monitor(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create components.\n",
    "deploy_op = comp.func_to_container_op(deploy, base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "inj_op = comp.func_to_container_op(data_injestion , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "transformation_op = comp.func_to_container_op(data_transformation, base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "predict_op = comp.func_to_container_op(predict, base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "monitor_op = comp.func_to_container_op(monitor, base_image = \"tensorflow/tensorflow:latest-gpu-py3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Kubeflow Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a client to enable communication with the Pipelines API server.\n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "    name='Sms Spam Classification Pipeline',\n",
    "    description=\n",
    "    'A machine learning pipeline that makes predictions on whether on not the sms content of a csv file is spam or not.'\n",
    ")\n",
    "# Define parameters to be fed into pipeline\n",
    "def spam_pipeline(data_path: str):\n",
    "\n",
    "    # Define volume to share data between components.\n",
    "    vop = dsl.VolumeOp(name=\"create_volume\",\n",
    "                       resource_name=\"data-volume\",\n",
    "                       size=\"1Gi\",\n",
    "                       modes=dsl.VOLUME_MODE_RWO)\n",
    "    \n",
    "    # Create deploy component.\n",
    "    deploy_container = deploy_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: vop.volume})\n",
    "\n",
    "    # Create data injestion component.\n",
    "    injestion_container = inj_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: deploy_container.pvolume})\n",
    "\n",
    "    # Create data transformation component.\n",
    "    transformation_container = transformation_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: injestion_container.pvolume})\n",
    "    # Create model training component.\n",
    "    predict_container = predict_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: transformation_container.pvolume})\n",
    "    \n",
    "    # Create model validation component.\n",
    "    monitor_container = monitor_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: predict_container.pvolume})\n",
    "    \n",
    "\n",
    "    # Print the result of the prediction\n",
    "    validation_result_container = dsl.ContainerOp(\n",
    "        name=\"print_validation_result\",\n",
    "        image='library/bash:4.4.23',\n",
    "        pvolumes={data_path: monitor_container.pvolume},\n",
    "        arguments=['cat', f'{data_path}/result.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = data_path\n",
    "\n",
    "pipeline_func = spam_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/kfp/dsl/_container_op.py:1028: FutureWarning: Please create reusable components instead of constructing ContainerOp instances directly. Reusable components are shareable, portable and have compatibility and support guarantees. Please see the documentation: https://www.kubeflow.org/docs/pipelines/sdk/component-development/#writing-your-component-definition-file The components can be created manually (or, in case of python, using kfp.components.create_component_from_func or func_to_container_op) and then loaded using kfp.components.load_component_from_file, load_component_from_uri or load_component_from_text: https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.components.html#kfp.components.load_component_from_file\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/2338d12a-7b68-4ee3-9559-310fb22820fb\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/1ab41975-53ac-4124-a093-8bf41203fb1c\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = 'spam_classification_kubeflow'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func, '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
